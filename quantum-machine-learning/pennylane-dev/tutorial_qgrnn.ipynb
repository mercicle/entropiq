{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Quantum Graph Recurrent Neural Network\n==========================================\n\n*Author: Jack Ceroni. Posted: 27 July 2020. Last updated: 25 March\n2021.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This demonstration investigates quantum graph recurrent neural networks\n(QGRNN), which are the quantum analogue of a classical graph recurrent\nneural network, and a subclass of the more general quantum graph neural\nnetwork ansatz. Both the QGNN and QGRNN were introduced in [this paper\n(2019)](https://arxiv.org/abs/1909.12264).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Idea\n========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A graph is defined as a set of *nodes* along with a set of **edges**,\nwhich represent connections between nodes. Information can be encoded\ninto graphs by assigning numbers to nodes and edges, which we call\n**weights**. It is usually convenient to think of a graph visually:\n\n![image](../demonstrations/qgrnn/graph.png){width=\"70%\"}\n\nIn recent years, the concept of a [graph neural\nnetwork](https://arxiv.org/abs/1812.08434) (GNN) has been receiving a\nlot of attention from the machine learning community. A GNN seeks to\nlearn a representation (a mapping of data into a low-dimensional vector\nspace) of a given graph with feature vectors assigned to nodes and\nedges. Each of the vectors in the learned representation preserves not\nonly the features, but also the overall topology of the graph, i.e.,\nwhich nodes are connected by edges. The quantum graph neural network\nattempts to do something similar, but for features that are\nquantum-mechanical; for instance, a collection of quantum states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the class of qubit Hamiltonians that are *quadratic*, meaning\nthat the terms of the Hamiltonian represent either interactions between\ntwo qubits, or the energy of individual qubits. This class of\nHamiltonians is naturally described by graphs, with second-order terms\nbetween qubits corresponding to weighted edges between nodes, and\nfirst-order terms corresponding to node weights.\n\nA well known example of a quadratic Hamiltonian is the transverse-field\nIsing model, which is defined as\n\n$$\\hat{H}_{\\text{Ising}}(\\boldsymbol\\theta) \\ = \\ \\displaystyle\\sum_{(i, j) \\in E}\n\\theta_{ij}^{(1)} Z_{i} Z_{j} \\ + \\ \\displaystyle\\sum_{i} \\theta_{i}^{(2)} Z_{i} \\ + \\\n\\displaystyle\\sum_{i} X_{i},$$\n\nwhere $\\boldsymbol\\theta \\ = \\ \\{\\theta^{(1)}, \\ \\theta^{(2)}\\}$. In\nthis Hamiltonian, the set $E$ that determines which pairs of qubits have\n$ZZ$ interactions can be represented by the set of edges for some graph.\nWith the qubits as nodes, this graph is called the *interaction graph*.\nThe $\\theta^{(1)}$ parameters correspond to the edge weights and the\n$\\theta^{(2)}$ parameters correspond to weights on the nodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This result implies that we can think about *quantum circuits* with\ngraph-theoretic properties. Recall that the time-evolution operator with\nrespect to some Hamiltonian $H$ is defined as:\n\n$$U \\ = \\ e^{-it H}.$$\n\nThus, we have a clean way of taking quadratic Hamiltonians and turning\nthem into unitaries (quantum circuits) that preserve the same\ncorrespondance to a graph. In the case of the Ising Hamiltonian, we\nhave:\n\n$$U_{\\text{Ising}} \\ = \\ e^{-it \\hat{H}_{\\text{Ising}} (\\boldsymbol\\theta)} \\ = \\ \\exp \\Big[ -it\n\\Big( \\displaystyle\\sum_{(i, j) \\in E} \\theta_{ij}^{(1)} Z_{i} Z_{j} \\ + \\\n\\displaystyle\\sum_{i} \\theta_{i}^{(2)} Z_{i} \\ + \\ \\displaystyle\\sum_{i} X_{i} \\Big) \\Big]$$\n\nIn general, this kind of unitary is very difficult to implement on a\nquantum computer. However, we can approximate it using the\n[Trotter-Suzuki\ndecomposition](https://en.wikipedia.org/wiki/Time-evolving_block_decimation#The_Suzuki-Trotter_expansion):\n\n$$\\exp \\Big[ -it \\Big( \\displaystyle\\sum_{(i, j) \\in E} \\theta_{ij}^{(1)} Z_{i} Z_{j} \\ + \\\n\\displaystyle\\sum_{i} \\theta_{i}^{(2)} Z_{i} \\ + \\ \\displaystyle\\sum_{i} X_{i} \\Big) \\Big]\n\\ \\approx \\ \\displaystyle\\prod_{k \\ = \\ 1}^{t / \\Delta} \\Bigg[ \\displaystyle\\prod_{j \\ = \\\n1}^{Q} e^{-i \\Delta \\hat{H}_{\\text{Ising}}^{j}(\\boldsymbol\\theta)} \\Bigg]$$\n\nwhere $\\hat{H}_{\\text{Ising}}^{j}(\\boldsymbol\\theta)$ is the $j$-th term\nof the Ising Hamiltonian and $\\Delta$ is some small number.\n\nThis circuit is a specific instance of the **Quantum Graph Recurrent\nNeural Network**, which in general is defined as a variational ansatz of\nthe form\n\n$$U_{H}(\\boldsymbol\\mu, \\ \\boldsymbol\\gamma) \\ = \\ \\displaystyle\\prod_{i \\ = \\ 1}^{P} \\Bigg[\n\\displaystyle\\prod_{j \\ = \\ 1}^{Q} e^{-i \\gamma_j H^{j}(\\boldsymbol\\mu)} \\Bigg],$$\n\nfor some parametrized quadratic Hamiltonian, $H(\\boldsymbol\\mu)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the QGRNN\n===============\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the QGRNN ansatz is equivalent to the approximate time evolution\nof some quadratic Hamiltonian, we can use it to learn the dynamics of a\nquantum system.\n\nContinuing with the Ising model example, let's imagine we have some\nsystem governed by $\\hat{H}_{\\text{Ising}}(\\boldsymbol\\alpha)$ for an\nunknown set of target parameters, $\\boldsymbol\\alpha$ and an unknown\ninteraction graph $G$. Let's also suppose we have access to copies of\nsome low-energy, non-ground state of the target Hamiltonian,\n$|\\psi_0\\rangle$. In addition, we have access to a collection of\ntime-evolved states,\n$\\{ |\\psi(t_1)\\rangle, \\ |\\psi(t_2)\\rangle, \\ ..., \\ |\\psi(t_N)\\rangle \\}$,\ndefined by:\n\n$$|\\psi(t_k)\\rangle \\ = \\ e^{-i t_k \\hat{H}_{\\text{Ising}}(\\boldsymbol\\alpha)} |\\psi_0\\rangle.$$\n\nWe call the low-energy states and the collection of time-evolved states\n*quantum data*. From here, we randomly pick a number of time-evolved\nstates from our collection. For any state that we choose, which is\nevolved to some time $t_k$, we compare it to\n\n$$U_{\\hat{H}_{\\text{Ising}}}(\\boldsymbol\\mu, \\ \\Delta) |\\psi_0\\rangle \\ \\approx \\ e^{-i t_k\n\\hat{H}_{\\text{Ising}}(\\boldsymbol\\mu)} |\\psi_0\\rangle.$$\n\nThis is done by feeding one of the copies of $|\\psi_0\\rangle$ into a\nquantum circuit with the QGRNN ansatz, with some guessed set of\nparameters $\\boldsymbol\\mu$ and a guessed interaction graph, $G'$. We\nthen use a classical optimizer to maximize the average \"similarity\"\nbetween the time-evolved states and the states prepared with the QGRNN.\n\nAs the QGRNN states becomes more similar to each time-evolved state for\neach sampled time, it follows that\n$\\boldsymbol\\mu \\ \\rightarrow \\ \\boldsymbol\\alpha$ and we are able to\nlearn the unknown parameters of the Hamiltonian.\n\n![A visual representation of one execution of the QGRNN for one piece of\nquantum data.](../demonstrations/qgrnn/qgrnn3.png){width=\"90%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning an Ising Model with the QGRNN\n======================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now attempt to use the QGRNN to learn the parameters corresponding to\nan arbitrary transverse-field Ising model Hamiltonian.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting Started\n===============\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin by importing the necessary dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport scipy\nimport networkx as nx\nimport copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also define some fixed values that are used throughout the\nsimulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qubit_number = 4\nqubits = range(qubit_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this simulation, we don't have quantum data readily available to pass\ninto the QGRNN, so we have to generate it ourselves. To do this, we must\nhave knowledge of the target interaction graph and the target\nHamiltonian.\n\nLet us use the following cyclic graph as the target interaction graph of\nthe Ising Hamiltonian:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ising_graph = nx.cycle_graph(qubit_number)\n\nprint(f\"Edges: {ising_graph.edges}\")\nnx.draw(ising_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then initialize the \u201cunknown\u201d target parameters that describe the\ntarget Hamiltonian,\n$\\boldsymbol\\alpha \\ = \\ \\{\\alpha^{(1)}, \\ \\alpha^{(2)}\\}$. Recall from\nthe introduction that we have defined our parametrized Ising Hamiltonian\nto be of the form:\n\n$$\\hat{H}_{\\text{Ising}}(\\boldsymbol\\theta) \\ = \\ \\displaystyle\\sum_{(i, j) \\in E}\n\\theta_{ij}^{(1)} Z_{i} Z_{j} \\ + \\ \\displaystyle\\sum_{i} \\theta_{i}^{(2)} Z_{i} \\ + \\\n\\displaystyle\\sum_{i} X_{i},$$\n\nwhere $E$ is the set of edges in the interaction graph, and $X_i$ and\n$Z_i$ are the Pauli-X and Pauli-Z on the $i$-th qubit.\n\nFor this tutorial, we choose the target parameters by sampling from a\nuniform probability distribution ranging from $-2$ to $2$, with\ntwo-decimal precision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_weights = [0.56, 1.24, 1.67, -0.79]\ntarget_bias = [-1.44, -1.43, 1.18, -0.93]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In theory, these parameters can be any value we want, provided they are\nreasonably small enough that the QGRNN can reach them in a tractable\nnumber of optimization steps. In `matrix_params`, the first list\nrepresents the $ZZ$ interaction parameters and the second list\nrepresents the single-qubit $Z$ parameters.\n\nFinally, we use this information to generate the matrix form of the\nIsing model Hamiltonian in the computational basis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_hamiltonian_matrix(n_qubits, graph, weights, bias):\n\n    full_matrix = np.zeros((2 ** n_qubits, 2 ** n_qubits))\n\n    # Creates the interaction component of the Hamiltonian\n    for i, edge in enumerate(graph.edges):\n        interaction_term = 1\n        for qubit in range(0, n_qubits):\n            if qubit in edge:\n                interaction_term = np.kron(interaction_term, qml.PauliZ.matrix)\n            else:\n                interaction_term = np.kron(interaction_term, np.identity(2))\n        full_matrix += weights[i] * interaction_term\n\n    # Creates the bias components of the matrix\n    for i in range(0, n_qubits):\n        z_term = x_term = 1\n        for j in range(0, n_qubits):\n            if j == i:\n                z_term = np.kron(z_term, qml.PauliZ.matrix)\n                x_term = np.kron(x_term, qml.PauliX.matrix)\n            else:\n                z_term = np.kron(z_term, np.identity(2))\n                x_term = np.kron(x_term, np.identity(2))\n        full_matrix += bias[i] * z_term + x_term\n\n    return full_matrix\n\n\n# Prints a visual representation of the Hamiltonian matrix\nham_matrix = create_hamiltonian_matrix(qubit_number, ising_graph, target_weights, target_bias)\nplt.matshow(ham_matrix, cmap=\"hot\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing Quantum Data\n======================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The collection of quantum data needed to run the QGRNN has two\ncomponents: (i) copies of a low-energy state, and (ii) a collection of\ntime-evolved states, each of which are simply the low-energy state\nevolved to different times. The following is a low-energy state of the\ntarget Hamiltonian:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "low_energy_state = [\n    (-0.054661080280306085 + 0.016713907320174026j),\n    (0.12290003656489545 - 0.03758500591109822j),\n    (0.3649337966440005 - 0.11158863596657455j),\n    (-0.8205175732627094 + 0.25093231967092877j),\n    (0.010369790825776609 - 0.0031706387262686003j),\n    (-0.02331544978544721 + 0.007129899300113728j),\n    (-0.06923183949694546 + 0.0211684344103713j),\n    (0.15566094863283836 - 0.04760201916285508j),\n    (0.014520590919500158 - 0.004441887836078486j),\n    (-0.032648113364535575 + 0.009988590222879195j),\n    (-0.09694382811137187 + 0.02965579457620536j),\n    (0.21796861485652747 - 0.06668776658411019j),\n    (-0.0027547112135013247 + 0.0008426289322652901j),\n    (0.006193695872468649 - 0.0018948418969390599j),\n    (0.018391279795405405 - 0.005625722994009138j),\n    (-0.041350974715649635 + 0.012650711602265649j),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This state can be obtained by using a decoupled version of the\nVariational Quantum Eigensolver &lt;/demos/tutorial\\_vqe&gt; algorithm\n(VQE). Essentially, we choose a VQE ansatz such that the circuit cannot\nlearn the exact ground state, but it can get fairly close. Another way\nto arrive at the same result is to perform VQE with a reasonable ansatz,\nbut to terminate the algorithm before it converges to the ground state.\nIf we used the exact ground state $|\\psi_0\\rangle$, the time-dependence\nwould be trivial and the data would not provide enough information about\nthe Hamiltonian parameters.\n\nWe can verify that this is a low-energy state by numerically finding the\nlowest eigenvalue of the Hamiltonian and comparing it to the energy\nexpectation of this low-energy state:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res = np.vdot(low_energy_state, (ham_matrix @ low_energy_state))\nenergy_exp = np.real_if_close(res)\nprint(f\"Energy Expectation: {energy_exp}\")\n\n\nground_state_energy = np.real_if_close(min(np.linalg.eig(ham_matrix)[0]))\nprint(f\"Ground State Energy: {ground_state_energy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have in fact found a low-energy, non-ground state, as the energy\nexpectation is slightly greater than the energy of the true ground\nstate. This, however, is only half of the information we need. We also\nrequire a collection of time-evolved, low-energy states. Evolving the\nlow-energy state forward in time is fairly straightforward: all we have\nto do is multiply the initial state by a time-evolution unitary. This\noperation can be defined as a custom gate in PennyLane:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def state_evolve(hamiltonian, qubits, time):\n\n    U = scipy.linalg.expm(-1j * hamiltonian * time)\n    qml.QubitUnitary(U, wires=qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We don't actually generate time-evolved quantum data quite yet, but we\nnow have all the pieces required for its preparation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning the Hamiltonian\n========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the quantum data defined, we are able to construct the QGRNN and\nlearn the target Hamiltonian. Each of the exponentiated Hamiltonians in\nthe QGRNN ansatz, $\\hat{H}^{j}_{\\text{Ising}}(\\boldsymbol\\mu)$, are the\n$ZZ$, $Z$, and $X$ terms from the Ising Hamiltonian. This gives:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def qgrnn_layer(weights, bias, qubits, graph, trotter_step):\n\n    # Applies a layer of RZZ gates (based on a graph)\n    for i, edge in enumerate(graph.edges):\n        qml.MultiRZ(2 * weights[i] * trotter_step, wires=(edge[0], edge[1]))\n\n    # Applies a layer of RZ gates\n    for i, qubit in enumerate(qubits):\n        qml.RZ(2 * bias[i] * trotter_step, wires=qubit)\n\n    # Applies a layer of RX gates\n    for qubit in qubits:\n        qml.RX(2 * trotter_step, wires=qubit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As was mentioned in the first section, the QGRNN has two registers. In\none register, some piece of quantum data $|\\psi(t)\\rangle$ is prepared\nand in the other we have\n$U_{H}(\\boldsymbol\\mu, \\ \\Delta) |\\psi_0\\rangle$. We need a way to\nmeasure the similarity between these states. This can be done by using\nthe fidelity, which is simply the modulus squared of the inner product\nbetween the states,\n$| \\langle \\psi(t) | U_{H}(\\Delta, \\ \\boldsymbol\\mu) |\\psi_0\\rangle |^2$.\nTo calculate this value, we use a [SWAP\ntest](https://en.wikipedia.org/wiki/Swap_test) between the registers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def swap_test(control, register1, register2):\n\n    qml.Hadamard(wires=control)\n    for reg1_qubit, reg2_qubit in zip(register1, register2):\n        qml.CSWAP(wires=(control, reg1_qubit, reg2_qubit))\n    qml.Hadamard(wires=control)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After performing this procedure, the value returned from a measurement\nof the circuit is $\\langle Z \\rangle$, with respect to the `control`\nqubit. The probability of measuring the $|0\\rangle$ state in this\ncontrol qubit is related to both the fidelity between registers and\n$\\langle Z \\rangle$. Thus, with a bit of algebra, we find that\n$\\langle Z \\rangle$ is equal to the fidelity.\n\nBefore creating the full QGRNN and the cost function, we define a few\nmore fixed values. Among these is a \"guessed\" interaction graph, which\nwe set to be a [complete\ngraph](https://en.wikipedia.org/wiki/Complete_graph). This choice is\nmotivated by the fact that any target interaction graph will be a\nsubgraph of this initial guess. Part of the idea behind the QGRNN is\nthat we don\u2019t know the interaction graph, and it has to be learned. In\nthis case, the graph is learned *automatically* as the target parameters\nare optimized. The $\\boldsymbol\\mu$ parameters that correspond to edges\nthat don't exist in the target graph will simply approach $0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Defines some fixed values\n\nreg1 = tuple(range(qubit_number))  # First qubit register\nreg2 = tuple(range(qubit_number, 2 * qubit_number))  # Second qubit register\n\ncontrol = 2 * qubit_number  # Index of control qubit\ntrotter_step = 0.01  # Trotter step size\n\n# Defines the interaction graph for the new qubit system\n\nnew_ising_graph = nx.complete_graph(reg2)\n\nprint(f\"Edges: {new_ising_graph.edges}\")\nnx.draw(new_ising_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this done, we implement the QGRNN circuit for some given time\nvalue:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def qgrnn(weights, bias, time=None):\n\n    # Prepares the low energy state in the two registers\n    qml.QubitStateVector(np.kron(low_energy_state, low_energy_state), wires=reg1 + reg2)\n\n    # Evolves the first qubit register with the time-evolution circuit to\n    # prepare a piece of quantum data\n    state_evolve(ham_matrix, reg1, time)\n\n    # Applies the QGRNN layers to the second qubit register\n    depth = time / trotter_step  # P = t/Delta\n    for _ in range(0, int(depth)):\n        qgrnn_layer(weights, bias, reg2, new_ising_graph, trotter_step)\n\n    # Applies the SWAP test between the registers\n    swap_test(control, reg1, reg2)\n\n    # Returns the results of the SWAP test\n    return qml.expval(qml.PauliZ(control))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the full QGRNN circuit, but we still need to define a cost\nfunction. We know that\n$| \\langle \\psi(t) | U_{H}(\\boldsymbol\\mu, \\ \\Delta) |\\psi_0\\rangle |^2$\napproaches $1$ as the states become more similar and approaches $0$ as\nthe states become orthogonal. Thus, we choose to minimize the quantity\n$-| \\langle \\psi(t) | U_{H}(\\boldsymbol\\mu, \\ \\Delta) |\\psi_0\\rangle |^2$.\nSince we are interested in calculating this value for many different\npieces of quantum data, the final cost function is the average negative\nfidelity\\* between registers:\n\n$$\\mathcal{L}(\\boldsymbol\\mu, \\ \\Delta) \\ = \\ - \\frac{1}{N} \\displaystyle\\sum_{i \\ = \\ 1}^{N} |\n\\langle \\psi(t_i) | \\ U_{H}(\\boldsymbol\\mu, \\ \\Delta) \\ |\\psi_0\\rangle |^2,$$\n\nwhere we use $N$ pieces of quantum data.\n\nBefore creating the cost function, we must define a few more fixed\nvariables:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 15  # The number of pieces of quantum data that are used for each step\nmax_time = 0.1  # The maximum value of time that can be used for quantum data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the negative fidelity cost function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(seed=42)\n\ndef cost_function(weight_params, bias_params):\n\n    # Randomly samples times at which the QGRNN runs\n    times_sampled = rng.random(size=N) * max_time\n\n    # Cycles through each of the sampled times and calculates the cost\n    total_cost = 0\n    for dt in times_sampled:\n        result = qgrnn_qnode(weight_params, bias_params, time=dt)\n        total_cost += -1 * result\n\n    return total_cost / N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we set up for optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Defines the new device\nqgrnn_dev = qml.device(\"default.qubit\", wires=2 * qubit_number + 1)\n\n# Defines the new QNode\nqgrnn_qnode = qml.QNode(qgrnn, qgrnn_dev)\n\nsteps = 300\n\noptimizer = qml.AdamOptimizer(stepsize=0.5)\n\nweights = rng.random(size=len(new_ising_graph.edges)) - 0.5\nbias = rng.random(size=qubit_number) - 0.5\n\ninitial_weights = copy.copy(weights)\ninitial_bias = copy.copy(bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All that remains is executing the optimization loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(0, steps):\n    (weights, bias), cost = optimizer.step_and_cost(cost_function, weights, bias)\n\n    # Prints the value of the cost function\n    if i % 5 == 0:\n        print(f\"Cost at Step {i}: {cost}\")\n        print(f\"Weights at Step {i}: {weights}\")\n        print(f\"Bias at Step {i}: {bias}\")\n        print(\"---------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the learned parameters, we construct a visual representation of the\nHamiltonian to which they correspond and compare it to the target\nHamiltonian, and the initial guessed Hamiltonian:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_ham_matrix = create_hamiltonian_matrix(\n    qubit_number, nx.complete_graph(qubit_number), weights, bias\n)\n\ninit_ham = create_hamiltonian_matrix(\n    qubit_number, nx.complete_graph(qubit_number), initial_weights, initial_bias\n)\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 6))\n\naxes[0].matshow(ham_matrix, vmin=-7, vmax=7, cmap=\"hot\")\naxes[0].set_title(\"Target\", y=1.13)\n\naxes[1].matshow(init_ham, vmin=-7, vmax=7, cmap=\"hot\")\naxes[1].set_title(\"Initial\", y=1.13)\n\naxes[2].matshow(new_ham_matrix, vmin=-7, vmax=7, cmap=\"hot\")\naxes[2].set_title(\"Learned\", y=1.13)\n\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These images look very similar, indicating that the QGRNN has done a\ngood job learning the target Hamiltonian.\n\nWe can also look at the exact values of the target and learned\nparameters. Recall how the target interaction graph has $4$ edges while\nthe complete graph has $6$. Thus, as the QGRNN converges to the optimal\nsolution, the weights corresponding to edges $(1, 3)$ and $(2, 0)$ in\nthe complete graph should go to $0$, as this indicates that they have no\neffect, and effectively do not exist in the learned Hamiltonian.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We first pick out the weights of edges (1, 3) and (2, 0)\n# and then remove them from the list of target parameters\n\nweights_noedge = []\nweights_edge = []\nfor ii, edge in enumerate(new_ising_graph.edges):\n    if (edge[0] - qubit_number, edge[1] - qubit_number) in ising_graph.edges:\n        weights_edge.append(weights[ii])\n    else:\n        weights_noedge.append(weights[ii])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we print all of the weights:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Target parameters     Learned parameters\")\nprint(\"Weights\")\nprint(\"-\" * 41)\nfor ii_target, ii_learned in zip(target_weights, weights_edge):\n    print(f\"{ii_target : <20}|{ii_learned : >20}\")\n\nprint(\"\\nBias\")\nprint(\"-\"*41)\nfor ii_target, ii_learned in zip(target_bias, bias):\n    print(f\"{ii_target : <20}|{ii_learned : >20}\")\n\nprint(f\"\\nNon-Existing Edge Parameters: {[val.unwrap() for val in weights_noedge]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The weights of edges $(1, 3)$ and $(2, 0)$ are very close to $0$,\nindicating we have learned the cycle graph from the complete graph. In\naddition, the remaining learned weights are fairly close to those of the\ntarget Hamiltonian. Thus, the QGRNN is functioning properly, and has\nlearned the target Ising Hamiltonian to a high degree of accuracy!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n\n1.  Verdon, G., McCourt, T., Luzhnica, E., Singh, V., Leichenauer, S., &\n    Hidary, J. (2019). Quantum Graph Neural Networks. arXiv preprint\n    [arXiv:1909.12264](https://arxiv.org/abs/1909.12264).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}