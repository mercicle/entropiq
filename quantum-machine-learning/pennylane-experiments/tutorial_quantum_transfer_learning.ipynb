{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum transfer learning\n=========================\n\n*Author: PennyLane dev team. Last updated: 28 Jan 2021.*\n\nIn this tutorial we apply a machine learning method, known as *transfer\nlearning*, to an image classifier based on a hybrid classical-quantum\nnetwork.\n\nThis example follows the general structure of the PyTorch [tutorial on\ntransfer\nlearning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\nby Sasank Chilamkurthy, with the crucial difference of using a quantum\ncircuit to perform the final classification task.\n\nMore details on this topic can be found in the research paper \\[1\\]\n([Mari et al. (2019)](https://arxiv.org/abs/1912.08278)).\n\nIntroduction\n------------\n\nTransfer learning is a well-established technique for training\nartificial neural networks (see e.g., Ref. \\[2\\]), which is based on the\ngeneral intuition that if a pre-trained network is good at solving a\ngiven problem, then, with just a bit of additional training, it can be\nused to also solve a different but related problem.\n\nAs discussed in Ref. \\[1\\], this idea can be formalized in terms of two\nabstract networks $A$ and $B$, independently from their quantum or\nclassical physical nature.\n\n|\n\n![](../demonstrations/quantum_transfer_learning/transfer_learning_general.png)\n\n|\n\nAs sketched in the above figure, one can give the following **general\ndefinition of the transfer learning method**:\n\n1.  Take a network $A$ that has been pre-trained on a dataset $D_A$ and\n    for a given task $T_A$.\n2.  Remove some of the final layers. In this way, the resulting\n    truncated network $A'$ can be used as a feature extractor.\n3.  Connect a new trainable network $B$ at the end of the pre-trained\n    network $A'$.\n4.  Keep the weights of $A'$ constant, and train the final block $B$\n    with a new dataset $D_B$ and/or for a new task of interest $T_B$.\n\nWhen dealing with hybrid systems, depending on the physical nature\n(classical or quantum) of the networks $A$ and $B$, one can have\ndifferent implementations of transfer learning as\n\nsummarized in following table:\n\n|\n\n  -------------------------------------------------------------------------\n  Network A   Network B   Transfer learning scheme\n  ----------- ----------- -------------------------------------------------\n  Classical   Classical   CC - Standard classical method. See e.g., Ref.\n                          \\[2\\].\n\n  Classical   Quantum     CQ - **Hybrid model presented in this tutorial.**\n\n  Quantum     Classical   QC - Model studied in Ref. \\[1\\].\n\n  Quantum     Quantum     QQ - Model studied in Ref. \\[1\\].\n  -------------------------------------------------------------------------\n\nClassical-to-quantum transfer learning\n--------------------------------------\n\nWe focus on the CQ transfer learning scheme discussed in the previous\nsection and we give a specific example.\n\n1.  As pre-trained network $A$ we use **ResNet18**, a deep residual\n    neural network introduced by Microsoft in Ref. \\[3\\], which is\n    pre-trained on the *ImageNet* dataset.\n2.  After removing its final layer we obtain $A'$, a pre-processing\n    block which maps any input high-resolution image into 512\n    abstract features.\n3.  Such features are classified by a 4-qubit \"dressed quantum circuit\"\n    $B$, i.e., a variational quantum circuit sandwiched between two\n    classical layers.\n4.  The hybrid model is trained, keeping $A'$ constant, on the\n    *Hymenoptera* dataset (a small subclass of ImageNet) containing\n    images of *ants* and *bees*.\n\nA graphical representation of the full data processing pipeline is given\nin the figure below.\n\n![](../demonstrations/quantum_transfer_learning/transfer_learning_c2q.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "General setup\n=============\n\n> **note**\n>\n> To use the PyTorch interface in PennyLane, you must first [install\n> PyTorch](https://pytorch.org/get-started/locally/#start-locally).\n\nIn addition to *PennyLane*, we will also need some standard *PyTorch*\nlibraries and the plotting library *matplotlib*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some parts of this code are based on the Python script:\n# https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py\n# License: BSD\n\nimport time\nimport os\nimport copy\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, transforms\n\n# Pennylane\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# OpenMP: number of parallel threads.\nos.environ[\"OMP_NUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting of the main hyper-parameters of the model\n=================================================\n\n> **note**\n\n> To reproduce the results of Ref. \\[1\\], `num_epochs` should be set to\n> `30` which may take a long time. We suggest to first try with\n> `num_epochs=1` and, if everything runs smoothly, increase it to a\n> larger value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_qubits = 4                # Number of qubits\nstep = 0.0004               # Learning rate\nbatch_size = 4              # Number of samples for each training step\nnum_epochs = 1              # Number of training epochs\nq_depth = 6                 # Depth of the quantum circuit (number of variational layers)\ngamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\nq_delta = 0.01              # Initial spread of random quantum weights\nstart_time = time.time()    # Start of the computation timer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize a PennyLane device with a `default.qubit` backend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=n_qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We configure PyTorch to use CUDA only if available. Otherwise the CPU is\nused.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset loading\n===============\n\n> **note**\n>\n> The dataset containing images of *ants* and *bees* can be downloaded\n>\n> :   [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)\n>     and should be extracted in the subfolder\n>     `../_data/hymenoptera_data`.\n>\nThis is a very small dataset (roughly 250 images), too small for\ntraining from scratch a classical or quantum model, however it is enough\nwhen using *transfer learning* approach.\n\nThe PyTorch packages `torchvision` and `torch.utils.data` are used for\nloading the dataset and performing standard preliminary image\noperations: resize, center, crop, normalize, *etc.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n    \"train\": transforms.Compose(\n        [\n            # transforms.RandomResizedCrop(224),     # uncomment for data augmentation\n            # transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            # Normalize input channels using mean values and standard deviations of ImageNet.\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    ),\n    \"val\": transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    ),\n}\n\ndata_dir = \"../_data/hymenoptera_data\"\nimage_datasets = {\n    x if x == \"train\" else \"validation\": datasets.ImageFolder(\n        os.path.join(data_dir, x), data_transforms[x]\n    )\n    for x in [\"train\", \"val\"]\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"validation\"]}\nclass_names = image_datasets[\"train\"].classes\n\n# Initialize dataloader\ndataloaders = {\n    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n    for x in [\"train\", \"validation\"]\n}\n\n# function to plot images\ndef imshow(inp, title=None):\n    \"\"\"Display image from tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    # Inverse of the initial normalization operation.\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us show a batch of the test data, just to have an idea of the\nclassification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data\ninputs, classes = next(iter(dataloaders[\"validation\"]))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\ndataloaders = {\n    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n    for x in [\"train\", \"validation\"]\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variational quantum circuit\n===========================\n\nWe first define some quantum layers that will compose the quantum\ncircuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.CNOT(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.CNOT(wires=[i, i + 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the quantum circuit through the PennyLane qnode decorator\n.\n\nThe structure is that of a typical variational quantum circuit:\n\n-   **Embedding layer:** All qubits are first initialized in a balanced\n    superposition of *up* and *down* states, then they are rotated\n    according to the input parameters (local embedding).\n-   **Variational layers:** A sequence of trainable rotation layers and\n    constant entangling layers is applied.\n-   **Measurement layer:** For each qubit, the local expectation value\n    of the $Z$ operator is measured. This produces a classical output\n    vector, suitable for additional post-processing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    RY_layer(q_input_features)\n\n    # Sequence of trainable variational layers\n    for k in range(q_depth):\n        entangling_layer(n_qubits)\n        RY_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dressed quantum circuit\n=======================\n\nWe can now define a custom `torch.nn.Module` representing a *dressed*\nquantum circuit.\n\nThis is a concatenation of:\n\n-   A classical pre-processing layer (`nn.Linear`).\n-   A classical activation function (`torch.tanh`).\n-   A constant `np.pi/2.0` scaling.\n-   The previously defined quantum circuit (`quantum_net`).\n-   A classical post-processing layer (`nn.Linear`).\n\nThe input of the module is a batch of vectors with 512 real parameters\n(features) and the output is a batch of vectors with two real outputs\n(associated with the two classes of images: *ants* and *bees*).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n\n        super().__init__()\n        self.pre_net = nn.Linear(512, n_qubits)\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n        self.post_net = nn.Linear(n_qubits, 2)\n\n    def forward(self, input_features):\n        \"\"\"\n        Defining how tensors are supposed to move through the *dressed* quantum\n        net.\n        \"\"\"\n\n        # obtain the input features for the quantum circuit\n        # by reducing the feature dimension from 512 to 4\n        pre_out = self.pre_net(input_features)\n        q_in = torch.tanh(pre_out) * np.pi / 2.0\n\n        # Apply the quantum circuit to each element of the batch and append to q_out\n        q_out = torch.Tensor(0, n_qubits)\n        q_out = q_out.to(device)\n        for elem in q_in:\n            q_out_elem = quantum_net(elem, self.q_params).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))\n\n        # return the two-dimensional prediction from the postprocessing layer\n        return self.post_net(q_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hybrid classical-quantum model\n==============================\n\nWe are finally ready to build our full hybrid classical-quantum network.\nWe follow the *transfer learning* approach:\n\n1.  First load the classical pre-trained network *ResNet18* from the\n    `torchvision.models` zoo.\n2.  Freeze all the weights since they should not be trained.\n3.  Replace the last fully connected layer with our trainable dressed\n    quantum circuit (`DressedQuantumNet`).\n\n> **note**\n\n> The *ResNet18* model is automatically downloaded by PyTorch and it may\n> take several minutes (only the first time).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_hybrid = torchvision.models.resnet18(pretrained=True)\n\nfor param in model_hybrid.parameters():\n    param.requires_grad = False\n\n\n# Notice that model_hybrid.fc is the last layer of ResNet18\nmodel_hybrid.fc = DressedQuantumNet()\n\n# Use CUDA or CPU according to the \"device\" object.\nmodel_hybrid = model_hybrid.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and results\n====================\n\nBefore training the network we need to specify the *loss* function.\n\nWe use, as usual in classification problem, the *cross-entropy* which is\ndirectly available within `torch.nn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also initialize the *Adam optimizer* which is called at each training\nstep in order to update the weights of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We schedule to reduce the learning rate by a factor of\n`gamma_lr_scheduler` every 10 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp_lr_scheduler = lr_scheduler.StepLR(\n    optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What follows is a training function that will be called later. This\nfunction should return a trained model that can be used to make\npredictions (classifications).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_loss = 10000.0  # Large arbitrary number\n    best_acc_train = 0.0\n    best_loss_train = 10000.0  # Large arbitrary number\n    print(\"Training started:\")\n\n    for epoch in range(num_epochs):\n\n        # Each epoch has a training and validation phase\n        for phase in [\"train\", \"validation\"]:\n            if phase == \"train\":\n                # Set model to training mode\n                model.train()\n            else:\n                # Set model to evaluate mode\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            n_batches = dataset_sizes[phase] // batch_size\n            it = 0\n            for inputs, labels in dataloaders[phase]:\n                since_batch = time.time()\n                batch_size_ = len(inputs)\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n\n                # Track/compute gradient and make an optimization step only when training\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n\n                # Print iteration results\n                running_loss += loss.item() * batch_size_\n                batch_corrects = torch.sum(preds == labels.data).item()\n                running_corrects += batch_corrects\n                print(\n                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n                        phase,\n                        epoch + 1,\n                        num_epochs,\n                        it + 1,\n                        n_batches + 1,\n                        time.time() - since_batch,\n                    ),\n                    end=\"\\r\",\n                    flush=True,\n                )\n                it += 1\n\n            # Print epoch results\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            print(\n                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n                    \"train\" if phase == \"train\" else \"validation  \",\n                    epoch + 1,\n                    num_epochs,\n                    epoch_loss,\n                    epoch_acc,\n                )\n            )\n\n            # Check if this is the best model wrt previous epochs\n            if phase == \"validation\" and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == \"validation\" and epoch_loss < best_loss:\n                best_loss = epoch_loss\n            if phase == \"train\" and epoch_acc > best_acc_train:\n                best_acc_train = epoch_acc\n            if phase == \"train\" and epoch_loss < best_loss_train:\n                best_loss_train = epoch_loss\n      \n            # Update learning rate\n            if phase == \"train\":\n                scheduler.step()\n\n    # Print final results\n    model.load_state_dict(best_model_wts)\n    time_elapsed = time.time() - since\n    print(\n        \"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n    )\n    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are ready to perform the actual training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_hybrid = train_model(\n    model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the model predictions\n=================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define a visualization function for a batch of test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6, fig_name=\"Predictions\"):\n    images_so_far = 0\n    _fig = plt.figure(fig_name)\n    model.eval()\n    with torch.no_grad():\n        for _i, (inputs, labels) in enumerate(dataloaders[\"validation\"]):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images // 2, 2, images_so_far)\n                ax.axis(\"off\")\n                ax.set_title(\"[{}]\".format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n                if images_so_far == num_images:\n                    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can run the previous function to see a batch of images with\nthe corresponding predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "visualize_model(model_hybrid, num_images=batch_size)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n\n\\[1\\] Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, and\nNathan Killoran. *Transfer learning in hybrid classical-quantum neural\nnetworks*. arXiv:1912.08278 (2019).\n\n\\[2\\] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and\nAndrew Y Ng. *Self-taught learning: transfer learning from unlabeled\ndata*. Proceedings of the 24th International Conference on Machine\nLearning\\*, 759\u2013766 (2007).\n\n\\[3\\] Kaiming He, Xiangyu Zhang, Shaoqing ren and Jian Sun. *Deep\nresidual learning for image recognition*. Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 770-778 (2016).\n\n\\[4\\] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin,\nCarsten Blank, Keri McKiernan, and Nathan Killoran. *PennyLane:\nAutomatic differentiation of hybrid quantum-classical computations*.\narXiv:1811.04968 (2018).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}